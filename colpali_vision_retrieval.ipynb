{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColPali + Qdrant Retrieval Pipeline for a Single PDF\n",
    "\n",
    "This notebook converts every page of a single PDF into images, embeds them with ColPali, and stores the embeddings in a local Qdrant instance for retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install required dependencies, import modules, and configure the compute device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet torch pdf2image Pillow matplotlib tqdm qdrant-client colpali-engine transformers qwen-vl-utils accelerate\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b8f20",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set paths and runtime parameters for the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81186661",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"./data/presentation.pdf\"  # Update with the actual PDF path\n",
    "OUTPUT_IMAGE_DIR = \"./data/pdf_pages\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"pdf_pages\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "output_dir_path = Path(OUTPUT_IMAGE_DIR)\n",
    "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PDF path: {os.path.abspath(PDF_PATH)}\")\n",
    "print(f\"Image output directory: {output_dir_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f3725",
   "metadata": {},
   "source": [
    "## 3. Convert PDF Pages to Images\n",
    "Render each page of the PDF to a PNG image and capture metadata for later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db46758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, output_dir: str) -> list[dict]:\n",
    "    \"\"\"Convert each page of the PDF into a PNG image on disk.\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images = convert_from_path(str(pdf_path))\n",
    "\n",
    "    page_records: list[dict] = []\n",
    "    base_name = pdf_path.stem\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        file_name = f\"page_{idx + 1:04d}.png\"\n",
    "        image_path = output_path / file_name\n",
    "        image.save(image_path, format=\"PNG\")\n",
    "\n",
    "        page_records.append(\n",
    "            {\n",
    "                \"page_index\": idx,\n",
    "                \"image_path\": str(image_path.resolve()),\n",
    "                \"pdf_file_name\": base_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return page_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5340a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = convert_pdf_to_images(PDF_PATH, OUTPUT_IMAGE_DIR)\n",
    "print(f\"Converted {len(pages)} pages to images.\")\n",
    "\n",
    "if pages:\n",
    "    with Image.open(pages[0][\"image_path\"]) as sample_image:\n",
    "        plt.figure(figsize=(6, 8))\n",
    "        plt.imshow(sample_image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Sample page 0 from {pages[0]['pdf_file_name']}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No pages were generated. Check the PDF path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049d236",
   "metadata": {},
   "source": [
    "## 4. Load ColPali Model\n",
    "Load the ColPali processor and model for generating image and text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vidore/colpali-v1.2\"\n",
    "processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "\n",
    "preferred_dtype = torch.bfloat16 if device.type in {\"cuda\", \"mps\"} else torch.float32\n",
    "\n",
    "try:\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(f\"Falling back to float32 due to: {error}\")\n",
    "    preferred_dtype = torch.float32\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded with dtype {preferred_dtype} on {device}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364809d1",
   "metadata": {},
   "source": [
    "## 5. Embed Page Images\n",
    "Batch process the saved page images through ColPali to create embeddings and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "all_embeddings_with_metadata=[]\n",
    "pdf_images = [Image.open(page_data[\"image_path\"]) for page_data in pages]\n",
    "data_loader = DataLoader(\n",
    "        dataset=pdf_images,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: processor.process_images(x),\n",
    "    )\n",
    "\n",
    "page_counter = 1\n",
    "for batch in tqdm(data_loader, desc=f\"Processing\"):\n",
    "    with torch.no_grad():\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        batch_embeddings = model(**batch)\n",
    "        batch_embeddings = list(torch.unbind(batch_embeddings.to(\"cpu\")))\n",
    "\n",
    "        for embedding in batch_embeddings:\n",
    "            all_embeddings_with_metadata.append({\n",
    "                \"embedding\": embedding,\n",
    "                \"page_id\": page_counter\n",
    "            })\n",
    "            page_counter += 1\n",
    "                \n",
    "all_embeddings_with_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a2b97",
   "metadata": {},
   "source": [
    "## 6. Initialize Qdrant Collection\n",
    "Create or recreate the Qdrant collection to store page embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "collection_name = \"pdf_pages\"\n",
    "\n",
    "# Create collection configured for multivector\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=128,\n",
    "        distance=models.Distance.COSINE,\n",
    "        multivector_config=models.MultiVectorConfig(\n",
    "            comparator=models.MultiVectorComparator.MAX_SIM\n",
    "        ),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569356a9",
   "metadata": {},
   "source": [
    "## 7. Upsert Embeddings into Qdrant\n",
    "Persist the vectors and metadata for each PDF page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for idx, embedding_record in enumerate(all_embeddings_with_metadata):\n",
    "    embedding_matrix = embedding_record['embedding'].to(torch.float32).cpu().numpy()\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=idx+1,\n",
    "                vector=embedding_matrix,\n",
    "                payload={\"image_id\": f\"{embedding_record['page_id']}\"},\n",
    "            )\n",
    "        ],\n",
    "        wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c18898",
   "metadata": {},
   "source": [
    "## 8. Retrieval Demo\n",
    "Search the Qdrant collection with a natural-language query and visualize matching pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from qdrant_client.models import QueryResponse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_query_embedding(query_text: str, model, processor):\n",
    "    batch_queries = processor.process_queries([query_text]).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**batch_queries)\n",
    "\n",
    "    return query_embeddings\n",
    "\n",
    "def search_similar(query_text: str, top_k: int = 5):\n",
    "    # 1. embed the user query into the SAME latent space as pages\n",
    "    query_embeddings = _get_query_embedding(query_text, model, processor)\n",
    "    query_matrx = query_embeddings[0].to(torch.float32).cpu().numpy()\n",
    "\n",
    "    # 2. vector similarity search in Qdrant\n",
    "    result: QueryResponse = client.query_points(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        query=query_matrx,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # 3. visualize + collect matches\n",
    "    matches = []\n",
    "    for rank, point in enumerate(result.points, start=1):\n",
    "        # add padding before image_id to match the saved file names        \n",
    "        image_path = f\"data/pdf_pages/page_{int(point.payload[\"image_id\"]):04d}.png\"\n",
    "\n",
    "        with Image.open(image_path) as image:\n",
    "            plt.figure(figsize=(6, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\n",
    "                f\"score={point.score:.4f} | page={point.payload[\"image_id\"]}\"\n",
    "            )\n",
    "            plt.show()\n",
    "\n",
    "        matches.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"score\": point.score,\n",
    "                \"image_path\": image_path,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8198c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query; update with a relevant question for your PDF\n",
    "matches = search_similar(\"Can I use Postgres for vector search\", top_k=3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f0a2c",
   "metadata": {},
   "source": [
    "## 9. Answer Generation Stub\n",
    "Outline how to hand retrieved images to a vision-language model for question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a40de",
   "metadata": {},
   "source": [
    "### Vision-Language Answering with Qwen2-VL\n",
    "\n",
    "After we retrieve the most relevant PDF page(s) using ColPali + Qdrant, we can ask a Vision-Language Model (VLM) to read that page image and answer the user's question.\n",
    "\n",
    "Flow:\n",
    "1. User asks a question.\n",
    "2. We embed the question, search Qdrant, and get back the top matching page image.\n",
    "3. We send that page image + the user's question into a local Qwen2-VL model.\n",
    "4. The model generates an answer using ONLY what it \"sees\" in that page.\n",
    "\n",
    "This section shows how to load Qwen2-VL locally and run a question-answer step against the best-matching page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0349acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "vlm_device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "vlm_dtype = torch.float16 if vlm_device.type == \"mps\" else torch.float32\n",
    "\n",
    "VLM_MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_NAME)\n",
    "vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    VLM_MODEL_NAME,\n",
    "    torch_dtype=vlm_dtype,\n",
    "    device_map=None,\n",
    ").to(vlm_device)\n",
    "\n",
    "vlm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ed92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "# make sure this import matches your source notebook\n",
    "from qwen_vl_utils import process_vision_info  \n",
    "\n",
    "def answer_question_with_vlm(query_text: str, retrieved_results, max_new_tokens: int = 500):\n",
    "    \"\"\"\n",
    "    Use the local Qwen2-VL model to answer `query_text`\n",
    "    based on the top retrieved PDF page image.\n",
    "\n",
    "    Assumes:\n",
    "    - vlm_model: loaded Qwen2-VL model (e.g. Qwen2VLForConditionalGeneration)\n",
    "    - vlm_processor: matching processor\n",
    "    - vlm_device: torch.device (\"mps\" or \"cpu\")\n",
    "    \"\"\"\n",
    "\n",
    "    if not retrieved_results:\n",
    "        return \"No results found to answer from.\"\n",
    "\n",
    "    # Get best match from vector search (Qdrant)\n",
    "    best = retrieved_results[0]\n",
    "    image_path = best[\"image_path\"]\n",
    "\n",
    "    # Load the page image from disk\n",
    "    page_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Build chat template with one image + the user question\n",
    "    chat_template = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": page_image},\n",
    "                {\"type\": \"text\", \"text\": query_text},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 1. Convert chat template to the actual LLM text prompt,\n",
    "    #    including special multimodal tokens, and append assistant prefix.\n",
    "    text_prompt = vlm_processor.apply_chat_template(\n",
    "        chat_template,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    # 2. Extract vision inputs in the format the model expects\n",
    "    image_inputs, _ = process_vision_info(chat_template)\n",
    "\n",
    "    # 3. Build final batch inputs for the model\n",
    "    inputs = vlm_processor(\n",
    "        text=[text_prompt],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # move tensors to correct device\n",
    "    inputs = {k: v.to(vlm_device) for k, v in inputs.items()}\n",
    "\n",
    "    # 4. Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = vlm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    # 5. Trim the prompt tokens so we only decode new answer tokens\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):]\n",
    "        for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n",
    "    ]\n",
    "\n",
    "    # 6. Decode to string\n",
    "    output_text = vlm_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    return output_text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ask something about the document\n",
    "user_query = \"Can I use Postgres for vector search? Is it built-in or do I need an extension?\"\n",
    "\n",
    "retrieved = search_similar(user_query, top_k=3)\n",
    "print(\"Top match page index:\", retrieved[0][\"image_path\"])\n",
    "\n",
    "final_answer = answer_question_with_vlm(user_query, retrieved)\n",
    "print(\"\\n=== Model Answer ===\\n\")\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182fabc",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "Release model resources so the notebook can be rerun without a kernel restart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05867a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del processor\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.mps.empty_cache()  # type: ignore[attr-defined]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del client\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "#del vlm_model\n",
    "#del vlm_processor\n",
    "#del vlm_device\n",
    "torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "import gc\n",
    "gc.collect()\n",
    "_ = gc.collect()\n",
    "print(\"Cleanup complete.\")\n",
    "print(\"VLM + retrieval models cleaned up.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Brew venv)",
   "language": "python",
   "name": "jupyter-brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
