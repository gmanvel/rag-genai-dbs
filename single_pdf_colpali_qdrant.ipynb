{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ColPali + Qdrant Retrieval Pipeline for a Single PDF\n",
        "\n",
        "This notebook converts every page of a single PDF into images, embeds them with ColPali, and stores the embeddings in a local Qdrant instance for retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n",
        "Install required dependencies, import modules, and configure the compute device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet torch pdf2image Pillow matplotlib tqdm qdrant-client colpali-engine\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "Set paths and runtime parameters for the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PDF_PATH = \"./data/sample.pdf\"  # Update with the actual PDF path\n",
        "OUTPUT_IMAGE_DIR = \"./data/pdf_pages\"\n",
        "QDRANT_URL = \"http://localhost:6333\"\n",
        "QDRANT_COLLECTION = \"pdf_pages\"\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "output_dir_path = Path(OUTPUT_IMAGE_DIR)\n",
        "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"PDF path: {os.path.abspath(PDF_PATH)}\")\n",
        "print(f\"Image output directory: {output_dir_path.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert PDF Pages to Images\n",
        "Render each page of the PDF to a PNG image and capture metadata for later steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_pdf_to_images(pdf_path: str, output_dir: str) -> list[dict]:\n",
        "    \"\"\"Convert each page of the PDF into a PNG image on disk.\"\"\"\n",
        "    pdf_path = Path(pdf_path)\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    images = convert_from_path(str(pdf_path))\n",
        "\n",
        "    page_records: list[dict] = []\n",
        "    base_name = pdf_path.stem\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        file_name = f\"page_{idx + 1:04d}.png\"\n",
        "        image_path = output_path / file_name\n",
        "        image.save(image_path, format=\"PNG\")\n",
        "\n",
        "        page_records.append(\n",
        "            {\n",
        "                \"page_index\": idx,\n",
        "                \"image_path\": str(image_path.resolve()),\n",
        "                \"pdf_file_name\": base_name,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return page_records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = convert_pdf_to_images(PDF_PATH, OUTPUT_IMAGE_DIR)\n",
        "print(f\"Converted {len(pages)} pages to images.\")\n",
        "\n",
        "if pages:\n",
        "    with Image.open(pages[0][\"image_path\"]) as sample_image:\n",
        "        plt.figure(figsize=(6, 8))\n",
        "        plt.imshow(sample_image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Sample page 0 from {pages[0]['pdf_file_name']}\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No pages were generated. Check the PDF path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load ColPali Model\n",
        "Load the ColPali processor and model for generating image and text embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"vidore/colpali-v1.2\"\n",
        "processor = ColPaliProcessor.from_pretrained(model_name)\n",
        "\n",
        "preferred_dtype = torch.bfloat16 if device.type in {\"cuda\", \"mps\"} else torch.float32\n",
        "\n",
        "try:\n",
        "    model = ColPali.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=preferred_dtype,\n",
        "    )\n",
        "except Exception as error:\n",
        "    print(f\"Falling back to float32 due to: {error}\")\n",
        "    preferred_dtype = torch.float32\n",
        "    model = ColPali.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=preferred_dtype,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded with dtype {preferred_dtype} on {device}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embed Page Images\n",
        "Batch process the saved page images through ColPali to create embeddings and metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_pages(pages: list[dict], model: ColPali, processor: ColPaliProcessor, batch_size: int):\n",
        "    \"\"\"Embed page images with ColPali and return metadata suitable for Qdrant.\"\"\"\n",
        "    embeddings: list[dict] = []\n",
        "\n",
        "    for start in tqdm(range(0, len(pages), batch_size), desc=\"Embedding pages\"):\n",
        "        batch = pages[start : start + batch_size]\n",
        "        images = []\n",
        "        for item in batch:\n",
        "            with Image.open(item[\"image_path\"]) as img:\n",
        "                images.append(img.convert(\"RGB\"))\n",
        "        batch_tensors = processor.process_images(images)\n",
        "        batch_tensors = {k: v.to(device) for k, v in batch_tensors.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_tensors)\n",
        "\n",
        "        if isinstance(outputs, torch.Tensor):\n",
        "            batch_embeddings = outputs\n",
        "        elif isinstance(outputs, (list, tuple)):\n",
        "            batch_embeddings = outputs[0]\n",
        "        elif hasattr(outputs, \"embeddings\"):\n",
        "            batch_embeddings = outputs.embeddings\n",
        "        elif hasattr(outputs, \"last_hidden_state\"):\n",
        "            batch_embeddings = outputs.last_hidden_state\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected model output type\")\n",
        "\n",
        "        for offset, embedding_tensor in enumerate(batch_embeddings):\n",
        "            embedding = embedding_tensor.detach().to(torch.float32).cpu().tolist()\n",
        "            page_info = batch[offset]\n",
        "            embeddings.append(\n",
        "                {\n",
        "                    \"id\": len(embeddings) + 1,\n",
        "                    \"embedding\": embedding,\n",
        "                    \"pdf_file_name\": page_info[\"pdf_file_name\"],\n",
        "                    \"page_index\": page_info[\"page_index\"],\n",
        "                    \"image_path\": page_info[\"image_path\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "page_embeddings = embed_pages(pages, model, processor, BATCH_SIZE)\n",
        "print(f\"Embedded {len(page_embeddings)} pages.\")\n",
        "\n",
        "if page_embeddings:\n",
        "    embedding_dim = len(page_embeddings[0][\"embedding\"])\n",
        "    print(f\"Embedding dimension: {embedding_dim}\")\n",
        "else:\n",
        "    raise RuntimeError(\"No embeddings generated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Qdrant Collection\n",
        "Create or recreate the Qdrant collection to store page embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(url=QDRANT_URL)\n",
        "vector_size = len(page_embeddings[0][\"embedding\"])\n",
        "\n",
        "client.recreate_collection(\n",
        "    collection_name=QDRANT_COLLECTION,\n",
        "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "print(f\"Collection '{QDRANT_COLLECTION}' ready with vector size {vector_size}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Upsert Embeddings into Qdrant\n",
        "Persist the vectors and metadata for each PDF page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "points = [\n",
        "    PointStruct(\n",
        "        id=item[\"id\"],\n",
        "        vector=item[\"embedding\"],\n",
        "        payload={\n",
        "            \"pdf_file_name\": item[\"pdf_file_name\"],\n",
        "            \"page_index\": item[\"page_index\"],\n",
        "            \"image_path\": item[\"image_path\"],\n",
        "        },\n",
        "    )\n",
        "    for item in page_embeddings\n",
        "]\n",
        "\n",
        "operation_info = client.upsert(collection_name=QDRANT_COLLECTION, points=points)\n",
        "\n",
        "print(f\"Upserted {len(points)} points into collection '{QDRANT_COLLECTION}'.\")\n",
        "if points:\n",
        "    print(\"Example payload:\", points[0].payload)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Retrieval Demo\n",
        "Search the Qdrant collection with a natural-language query and visualize matching pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_similar(query_text: str, top_k: int = 5):\n",
        "    processed_query = processor.process_queries([query_text])\n",
        "    processed_query = {k: v.to(device) for k, v in processed_query.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**processed_query)\n",
        "\n",
        "    if isinstance(outputs, torch.Tensor):\n",
        "        query_embedding_tensor = outputs[0]\n",
        "    elif isinstance(outputs, (list, tuple)):\n",
        "        query_embedding_tensor = outputs[0][0]\n",
        "    elif hasattr(outputs, \"embeddings\"):\n",
        "        query_embedding_tensor = outputs.embeddings[0]\n",
        "    elif hasattr(outputs, \"last_hidden_state\"):\n",
        "        query_embedding_tensor = outputs.last_hidden_state[0]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected model output type for query\")\n",
        "\n",
        "    query_vector = query_embedding_tensor.detach().to(torch.float32).cpu().tolist()\n",
        "\n",
        "    results = client.search(\n",
        "        collection_name=QDRANT_COLLECTION,\n",
        "        query_vector=query_vector,\n",
        "        limit=top_k,\n",
        "        with_payload=True,\n",
        "    )\n",
        "\n",
        "    matches = []\n",
        "    for rank, result in enumerate(results, start=1):\n",
        "        payload = result.payload\n",
        "        image_path = payload[\"image_path\"]\n",
        "        with Image.open(image_path) as image:\n",
        "            plt.figure(figsize=(6, 8))\n",
        "            plt.imshow(image)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"score={result.score:.4f} | page={payload['page_index']} | file={payload['pdf_file_name']}\")\n",
        "            plt.show()\n",
        "\n",
        "        matches.append(\n",
        "            {\n",
        "                \"rank\": rank,\n",
        "                \"score\": result.score,\n",
        "                \"payload\": payload,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example query; update with a relevant question for your PDF\n",
        "# matches = search_similar(\"What is the main topic of this document?\", top_k=3)\n",
        "# matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Answer Generation Stub\n",
        "Outline how to hand retrieved images to a vision-language model for question answering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_question_with_vlm(query_text: str, retrieved_results):\n",
        "    \"\"\"Pseudocode for sending top results to a VLM such as Qwen2-VL.\"\"\"\n",
        "    # Steps:\n",
        "    # 1. Select the best result, e.g., retrieved_results[0].\n",
        "    # 2. Load the corresponding page image: Image.open(result['payload']['image_path']).\n",
        "    # 3. Initialize the VLM (e.g., Qwen2-VL) and format the prompt with the image and query_text.\n",
        "    # 4. Generate the answer text from the VLM.\n",
        "    # 5. Return the generated answer string.\n",
        "    raise NotImplementedError(\"Integrate with your preferred VLM here.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup\n",
        "Release model resources so the notebook can be rerun without a kernel restart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del processor\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del page_embeddings\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.mps.empty_cache()  # type: ignore[attr-defined]\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import gc\n",
        "_ = gc.collect()\n",
        "\n",
        "print(\"Cleanup complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}