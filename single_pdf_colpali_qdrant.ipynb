{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColPali + Qdrant Retrieval Pipeline for a Single PDF\n",
    "\n",
    "This notebook converts every page of a single PDF into images, embeds them with ColPali, and stores the embeddings in a local Qdrant instance for retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install required dependencies, import modules, and configure the compute device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet torch pdf2image Pillow matplotlib tqdm qdrant-client colpali-engine transformers accelerate\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b8f20",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set paths and runtime parameters for the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81186661",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"./data/presentation.pdf\"  # Update with the actual PDF path\n",
    "OUTPUT_IMAGE_DIR = \"./data/pdf_pages\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"pdf_pages\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "output_dir_path = Path(OUTPUT_IMAGE_DIR)\n",
    "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PDF path: {os.path.abspath(PDF_PATH)}\")\n",
    "print(f\"Image output directory: {output_dir_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f3725",
   "metadata": {},
   "source": [
    "## 3. Convert PDF Pages to Images\n",
    "Render each page of the PDF to a PNG image and capture metadata for later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db46758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, output_dir: str) -> list[dict]:\n",
    "    \"\"\"Convert each page of the PDF into a PNG image on disk.\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images = convert_from_path(str(pdf_path))\n",
    "\n",
    "    page_records: list[dict] = []\n",
    "    base_name = pdf_path.stem\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        file_name = f\"page_{idx + 1:04d}.png\"\n",
    "        image_path = output_path / file_name\n",
    "        image.save(image_path, format=\"PNG\")\n",
    "\n",
    "        page_records.append(\n",
    "            {\n",
    "                \"page_index\": idx,\n",
    "                \"image_path\": str(image_path.resolve()),\n",
    "                \"pdf_file_name\": base_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return page_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5340a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = convert_pdf_to_images(PDF_PATH, OUTPUT_IMAGE_DIR)\n",
    "print(f\"Converted {len(pages)} pages to images.\")\n",
    "\n",
    "if pages:\n",
    "    with Image.open(pages[0][\"image_path\"]) as sample_image:\n",
    "        plt.figure(figsize=(6, 8))\n",
    "        plt.imshow(sample_image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Sample page 0 from {pages[0]['pdf_file_name']}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No pages were generated. Check the PDF path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049d236",
   "metadata": {},
   "source": [
    "## 4. Load ColPali Model\n",
    "Load the ColPali processor and model for generating image and text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vidore/colpali-v1.2\"\n",
    "processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "\n",
    "preferred_dtype = torch.bfloat16 if device.type in {\"cuda\", \"mps\"} else torch.float32\n",
    "\n",
    "try:\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(f\"Falling back to float32 due to: {error}\")\n",
    "    preferred_dtype = torch.float32\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded with dtype {preferred_dtype} on {device}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364809d1",
   "metadata": {},
   "source": [
    "## 5. Embed Page Images\n",
    "Batch process the saved page images through ColPali to create embeddings and metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a3038",
   "metadata": {},
   "source": [
    "### Page Embedding Strategy (Mean Pooling)\n",
    "\n",
    "ColPali produces multiple embedding vectors per page image. Conceptually, it splits the page into many visual regions / patches and gives us an embedding for each region.  \n",
    "That means for a single PDF page we may get something like 1,000+ vectors, each 128-dimensional.\n",
    "\n",
    "For our use case we want **page-level retrieval**:\n",
    "- A search query (text) should bring back the *whole* page image.\n",
    "- We don't need sub-page / bounding-box precision.\n",
    "\n",
    "To achieve that, we collapse all region embeddings for a page into **one** page embedding by taking the mean across all region vectors (mean pooling).  \n",
    "This gives us a single 128-dim vector that represents the entire page's visual/semantic content.\n",
    "\n",
    "We then:\n",
    "1. Store exactly one vector per page in Qdrant,\n",
    "2. Attach metadata (`page_index`, `pdf_file_name`, `image_path`),\n",
    "3. Later, when we search, Qdrant returns the most relevant page.\n",
    "\n",
    "This keeps Qdrant small (1 point per page) and aligns with our \"retrieve a whole page to display\" requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def embed_pages_mean_pooled(\n",
    "    pages,\n",
    "    model,\n",
    "    processor,\n",
    "    batch_size: int,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    pages: list[dict] from convert_pdf_to_images(), where each item has:\n",
    "        {\n",
    "            \"page_index\": int,\n",
    "            \"image_path\": str,\n",
    "            \"pdf_file_name\": str\n",
    "        }\n",
    "\n",
    "    Returns: list[dict] where each item is:\n",
    "        {\n",
    "            \"id\": int,                      # stable per page, 1-based\n",
    "            \"embedding\": list[float],       # pooled 128-dim vector\n",
    "            \"pdf_file_name\": str,\n",
    "            \"page_index\": int,\n",
    "            \"image_path\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # we'll iterate pages in batches\n",
    "    for start in range(0, len(pages), batch_size):\n",
    "        batch_slice = pages[start:start + batch_size]\n",
    "\n",
    "        # load images for this batch\n",
    "        pil_images = [Image.open(p[\"image_path\"]).convert(\"RGB\") for p in batch_slice]\n",
    "\n",
    "        # preprocess using the model's processor (same idea as in reference notebook)\n",
    "        batch_inputs = processor.process_images(pil_images)\n",
    "\n",
    "        # move tensors to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # model output shape is typically [B, num_regions, emb_dim]\n",
    "            model_outputs = model(**batch_inputs)  # tensor or dict-like depending on model\n",
    "            # if model returns dict-like, adapt accordingly:\n",
    "            if isinstance(model_outputs, dict):\n",
    "                page_region_embeddings = model_outputs[\"embeddings\"]\n",
    "            else:\n",
    "                page_region_embeddings = model_outputs\n",
    "\n",
    "            # mean pool across regions for each page\n",
    "            # page_region_embeddings: [B, R, D]\n",
    "            # pooled_page_embeddings: [B, D]\n",
    "            pooled_page_embeddings = page_region_embeddings.mean(dim=1).to(\"cpu\")\n",
    "\n",
    "        # build result objects\n",
    "        for i, pooled_vec in enumerate(pooled_page_embeddings):\n",
    "            src = batch_slice[i]\n",
    "\n",
    "            results.append({\n",
    "                \"id\": src[\"page_index\"] + 1,  # stable 1-based ID\n",
    "                \"embedding\": pooled_vec.tolist(),  # turn tensor->python list[float]\n",
    "                \"pdf_file_name\": src[\"pdf_file_name\"],\n",
    "                \"page_index\": src[\"page_index\"],\n",
    "                \"image_path\": str(Path(src[\"image_path\"]))\n",
    "            })\n",
    "\n",
    "        # cleanup per-batch to manage memory on Apple Silicon\n",
    "        del pil_images\n",
    "        del batch_inputs\n",
    "        del model_outputs\n",
    "        del page_region_embeddings\n",
    "        del pooled_page_embeddings\n",
    "        torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# actually run the embedding step\n",
    "page_embeddings = embed_pages_mean_pooled(\n",
    "    pages=pages,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Total pages embedded: {len(page_embeddings)}\")\n",
    "print(f\"Embedding dim of first page: {len(page_embeddings[0]['embedding'])}\")\n",
    "print(\"Sample metadata:\", {\n",
    "    \"page_index\": page_embeddings[0][\"page_index\"],\n",
    "    \"image_path\": page_embeddings[0][\"image_path\"],\n",
    "    \"pdf_file_name\": page_embeddings[0][\"pdf_file_name\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e62bd",
   "metadata": {},
   "source": [
    "## 6. Initialize Qdrant Collection\n",
    "Create or recreate the Qdrant collection to store page embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=QDRANT_URL)\n",
    "vector_size = len(page_embeddings[0][\"embedding\"])\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=QDRANT_COLLECTION,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"Collection '{QDRANT_COLLECTION}' ready with vector size {vector_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06fa25",
   "metadata": {},
   "source": [
    "## 7. Upsert Embeddings into Qdrant\n",
    "Persist the vectors and metadata for each PDF page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [\n",
    "    PointStruct(\n",
    "        id=item[\"id\"],\n",
    "        vector=item[\"embedding\"],\n",
    "        payload={\n",
    "            \"pdf_file_name\": item[\"pdf_file_name\"],\n",
    "            \"page_index\": item[\"page_index\"],\n",
    "            \"image_path\": item[\"image_path\"],\n",
    "        },\n",
    "    )\n",
    "    for item in page_embeddings\n",
    "]\n",
    "\n",
    "operation_info = client.upsert(collection_name=QDRANT_COLLECTION, points=points)\n",
    "\n",
    "print(f\"Upserted {len(points)} points into collection '{QDRANT_COLLECTION}'.\")\n",
    "if points:\n",
    "    print(\"Example payload:\", points[0].payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieval Demo\n",
    "Search the Qdrant collection with a natural-language query and visualize matching pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_query_embedding(query_text: str, model, processor, device: torch.device):\n",
    "    # 1. preprocess query for the text branch\n",
    "    query_inputs = processor.process_queries([query_text])\n",
    "    query_inputs = {k: v.to(device) for k, v in query_inputs.items()}\n",
    "\n",
    "    # 2. forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model(**query_inputs)\n",
    "\n",
    "    # 3. Normalize model output to a Tensor[seq_len, 128] or [1, seq_len, 128]\n",
    "    if isinstance(out, dict) and \"embeddings\" in out:\n",
    "        emb = out[\"embeddings\"]\n",
    "    elif hasattr(out, \"embeddings\"):\n",
    "        emb = out.embeddings\n",
    "    elif isinstance(out, torch.Tensor):\n",
    "        emb = out\n",
    "    else:\n",
    "        # fallback: try first element if it's a tuple/list\n",
    "        emb = out[0]\n",
    "\n",
    "    # At this point, based on your debug, emb is shaped like [23, 128]\n",
    "    # (or [1, 23, 128]). We want a SINGLE [128] vector, so we mean-pool\n",
    "    # across the sequence dimension.\n",
    "\n",
    "    if emb.dim() == 3:\n",
    "        # e.g. [batch=1, seq_len=23, 128] -> [seq_len=23, 128]\n",
    "        emb = emb[0]\n",
    "\n",
    "    # emb is now [seq_len, 128] == [23, 128] in your case.\n",
    "    pooled = emb.mean(dim=0)  # -> [128]\n",
    "\n",
    "    pooled = pooled.detach().to(torch.float32).cpu()\n",
    "    return pooled.tolist()  # final 128-dim vector\n",
    "\n",
    "\n",
    "\n",
    "def search_similar(query_text: str, top_k: int = 5):\n",
    "    # 1. embed the user query into the SAME latent space as pages\n",
    "    query_vector = _get_query_embedding(query_text, model, processor, device)\n",
    "\n",
    "    # 2. vector similarity search in Qdrant\n",
    "    results = client.search(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        query_vector=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    # 3. visualize + collect matches\n",
    "    matches = []\n",
    "    for rank, result in enumerate(results, start=1):\n",
    "        payload = result.payload\n",
    "        image_path = payload[\"image_path\"]\n",
    "\n",
    "        with Image.open(image_path) as image:\n",
    "            plt.figure(figsize=(6, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\n",
    "                f\"score={result.score:.4f} | page={payload['page_index']} | file={payload['pdf_file_name']}\"\n",
    "            )\n",
    "            plt.show()\n",
    "\n",
    "        matches.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"score\": result.score,\n",
    "                \"payload\": payload,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query; update with a relevant question for your PDF\n",
    "matches = search_similar(\"Can I use Postgres for vector search\", top_k=3)\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Answer Generation Stub\n",
    "Outline how to hand retrieved images to a vision-language model for question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vision-Language Answering with Qwen2-VL\n\nAfter we retrieve the most relevant PDF page(s) using ColPali + Qdrant, we can ask a Vision-Language Model (VLM) to read that page image and answer the user's question.\n\nWhy we need a VLM:\n- Our pipeline stores pages as images (not just plain text).\n- Sometimes OCR is missing or unreliable.\n- We want a natural-language answer grounded in the page content, including tables, diagrams, stamps, etc.\n\nFlow:\n1. User asks a question.\n2. We embed the question, search Qdrant, and get back the top matching page image.\n3. We send that page image + the user's question into a local Qwen2-VL model.\n4. The model generates an answer using ONLY what it \"sees\" in that page.\n\nThis section shows how to load Qwen2-VL locally and run a question-answer step against the best-matching page.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# Configure device for VLM\n",
    "if torch.backends.mps.is_available():\n",
    "    vlm_device = torch.device(\"mps\")\n",
    "else:\n",
    "    vlm_device = torch.device(\"cpu\")\n",
    "\n",
    "VLM_MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"  # keep this as a string variable so I can change it later\n",
    "\n",
    "vlm_dtype = torch.float16 if vlm_device.type == \"mps\" else torch.float32\n",
    "\n",
    "# Load processor / tokenizer / image processor\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_NAME)\n",
    "\n",
    "# Load model\n",
    "vlm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    VLM_MODEL_NAME,\n",
    "    torch_dtype=vlm_dtype,\n",
    "    device_map=None,\n",
    ")\n",
    "vlm_model = vlm_model.to(vlm_device)\n",
    "vlm_model.eval()\n",
    "\n",
    "print(\"VLM loaded on\", vlm_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_with_vlm(query_text: str, retrieved_results):\n",
    "    \"\"\"\n",
    "    Use the local Qwen VLM to answer the user's query by looking directly at\n",
    "    the most relevant PDF page image we retrieved from Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    if not retrieved_results:\n",
    "        return \"No results found to answer from.\"\n",
    "\n",
    "    best = retrieved_results[0]\n",
    "    image_path = best[\"payload\"][\"image_path\"]\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant that only uses information visible in the provided image.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query_text},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    prompt = vlm_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = vlm_processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(vlm_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = vlm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "\n",
    "    processed = vlm_processor.post_process_generation(\n",
    "        generated_ids=generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    answer = processed[0][\"generated_text\"]\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ask something about the document\n",
    "user_query = \"Summarize the main purpose of this document.\"\n",
    "\n",
    "retrieved = search_similar(user_query, top_k=3)\n",
    "print(\"Top match page index:\", retrieved[0][\"payload\"][\"page_index\"])\n",
    "print(\"Image path:\", retrieved[0][\"payload\"][\"image_path\"])\n",
    "\n",
    "final_answer = answer_question_with_vlm(user_query, retrieved)\n",
    "print(\"\\n=== Model Answer ===\\n\")\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "Release model resources so the notebook can be rerun without a kernel restart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del processor\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del page_embeddings\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.mps.empty_cache()  # type: ignore[attr-defined]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del client\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "del vlm_model\n",
    "del vlm_processor\n",
    "del vlm_device\n",
    "torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "import gc\n",
    "gc.collect()\n",
    "_ = gc.collect()\n",
    "print(\"Cleanup complete.\")\n",
    "print(\"VLM + retrieval models cleaned up.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Brew venv)",
   "language": "python",
   "name": "jupyter-brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
