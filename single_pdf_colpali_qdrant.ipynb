{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColPali + Qdrant Retrieval Pipeline for a Single PDF\n",
    "\n",
    "This notebook converts every page of a single PDF into images, embeds them with ColPali, and stores the embeddings in a local Qdrant instance for retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install required dependencies, import modules, and configure the compute device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet torch pdf2image Pillow matplotlib tqdm qdrant-client colpali-engine transformers accelerate\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set paths and runtime parameters for the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"./data/sample.pdf\"  # Update with the actual PDF path\n",
    "OUTPUT_IMAGE_DIR = \"./data/pdf_pages\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "QDRANT_COLLECTION = \"pdf_pages\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "output_dir_path = Path(OUTPUT_IMAGE_DIR)\n",
    "output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PDF path: {os.path.abspath(PDF_PATH)}\")\n",
    "print(f\"Image output directory: {output_dir_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert PDF Pages to Images\n",
    "Render each page of the PDF to a PNG image and capture metadata for later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, output_dir: str) -> list[dict]:\n",
    "    \"\"\"Convert each page of the PDF into a PNG image on disk.\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images = convert_from_path(str(pdf_path))\n",
    "\n",
    "    page_records: list[dict] = []\n",
    "    base_name = pdf_path.stem\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        file_name = f\"page_{idx + 1:04d}.png\"\n",
    "        image_path = output_path / file_name\n",
    "        image.save(image_path, format=\"PNG\")\n",
    "\n",
    "        page_records.append(\n",
    "            {\n",
    "                \"page_index\": idx,\n",
    "                \"image_path\": str(image_path.resolve()),\n",
    "                \"pdf_file_name\": base_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return page_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = convert_pdf_to_images(PDF_PATH, OUTPUT_IMAGE_DIR)\n",
    "print(f\"Converted {len(pages)} pages to images.\")\n",
    "\n",
    "if pages:\n",
    "    with Image.open(pages[0][\"image_path\"]) as sample_image:\n",
    "        plt.figure(figsize=(6, 8))\n",
    "        plt.imshow(sample_image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Sample page 0 from {pages[0]['pdf_file_name']}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No pages were generated. Check the PDF path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load ColPali Model\n",
    "Load the ColPali processor and model for generating image and text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vidore/colpali-v1.2\"\n",
    "processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "\n",
    "preferred_dtype = torch.bfloat16 if device.type in {\"cuda\", \"mps\"} else torch.float32\n",
    "\n",
    "try:\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(f\"Falling back to float32 due to: {error}\")\n",
    "    preferred_dtype = torch.float32\n",
    "    model = ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=preferred_dtype,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded with dtype {preferred_dtype} on {device}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embed Page Images\n",
    "Batch process the saved page images through ColPali to create embeddings and metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Embedding Strategy (Mean Pooling)\n",
    "\n",
    "ColPali produces multiple embedding vectors per page image. Conceptually, it splits the page into many visual regions / patches and gives us an embedding for each region.  \n",
    "That means for a single PDF page we may get something like 1,000+ vectors, each 128-dimensional.\n",
    "\n",
    "For our use case we want **page-level retrieval**:\n",
    "- A search query (text) should bring back the *whole* page image.\n",
    "- We don't need sub-page / bounding-box precision.\n",
    "\n",
    "To achieve that, we collapse all region embeddings for a page into **one** page embedding by taking the mean across all region vectors (mean pooling).  \n",
    "This gives us a single 128-dim vector that represents the entire page's visual/semantic content.\n",
    "\n",
    "We then:\n",
    "1. Store exactly one vector per page in Qdrant,\n",
    "2. Attach metadata (`page_index`, `pdf_file_name`, `image_path`),\n",
    "3. Later, when we search, Qdrant returns the most relevant page.\n",
    "\n",
    "This keeps Qdrant small (1 point per page) and aligns with our \"retrieve a whole page to display\" requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def embed_pages_mean_pooled(\n",
    "    pages,\n",
    "    model,\n",
    "    processor,\n",
    "    batch_size: int,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    pages: list[dict] from convert_pdf_to_images(), where each item has:\n",
    "        {\n",
    "            \"page_index\": int,\n",
    "            \"image_path\": str,\n",
    "            \"pdf_file_name\": str\n",
    "        }\n",
    "\n",
    "    Returns: list[dict] where each item is:\n",
    "        {\n",
    "            \"id\": int,                      # stable per page, 1-based\n",
    "            \"embedding\": list[float],       # pooled 128-dim vector\n",
    "            \"pdf_file_name\": str,\n",
    "            \"page_index\": int,\n",
    "            \"image_path\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # we'll iterate pages in batches\n",
    "    for start in range(0, len(pages), batch_size):\n",
    "        batch_slice = pages[start:start + batch_size]\n",
    "\n",
    "        # load images for this batch\n",
    "        pil_images = [Image.open(p[\"image_path\"]).convert(\"RGB\") for p in batch_slice]\n",
    "\n",
    "        # preprocess using the model's processor (same idea as in reference notebook)\n",
    "        batch_inputs = processor.process_images(pil_images)\n",
    "\n",
    "        # move tensors to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # model output shape is typically [B, num_regions, emb_dim]\n",
    "            model_outputs = model(**batch_inputs)  # tensor or dict-like depending on model\n",
    "            # if model returns dict-like, adapt accordingly:\n",
    "            if isinstance(model_outputs, dict):\n",
    "                page_region_embeddings = model_outputs[\"embeddings\"]\n",
    "            else:\n",
    "                page_region_embeddings = model_outputs\n",
    "\n",
    "            # mean pool across regions for each page\n",
    "            # page_region_embeddings: [B, R, D]\n",
    "            # pooled_page_embeddings: [B, D]\n",
    "            pooled_page_embeddings = page_region_embeddings.mean(dim=1).to(\"cpu\")\n",
    "\n",
    "        # build result objects\n",
    "        for i, pooled_vec in enumerate(pooled_page_embeddings):\n",
    "            src = batch_slice[i]\n",
    "\n",
    "            results.append({\n",
    "                \"id\": src[\"page_index\"] + 1,  # stable 1-based ID\n",
    "                \"embedding\": pooled_vec.tolist(),  # turn tensor->python list[float]\n",
    "                \"pdf_file_name\": src[\"pdf_file_name\"],\n",
    "                \"page_index\": src[\"page_index\"],\n",
    "                \"image_path\": str(Path(src[\"image_path\"]))\n",
    "            })\n",
    "\n",
    "        # cleanup per-batch to manage memory on Apple Silicon\n",
    "        del pil_images\n",
    "        del batch_inputs\n",
    "        del model_outputs\n",
    "        del page_region_embeddings\n",
    "        del pooled_page_embeddings\n",
    "        torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# actually run the embedding step\n",
    "page_embeddings = embed_pages_mean_pooled(\n",
    "    pages=pages,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Total pages embedded: {len(page_embeddings)}\")\n",
    "print(f\"Embedding dim of first page: {len(page_embeddings[0]['embedding'])}\")\n",
    "print(\"Sample metadata:\", {\n",
    "    \"page_index\": page_embeddings[0][\"page_index\"],\n",
    "    \"image_path\": page_embeddings[0][\"image_path\"],\n",
    "    \"pdf_file_name\": page_embeddings[0][\"pdf_file_name\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Qdrant Collection\n",
    "Create or recreate the Qdrant collection to store page embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=QDRANT_URL)\n",
    "vector_size = len(page_embeddings[0][\"embedding\"])\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=QDRANT_COLLECTION,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"Collection '{QDRANT_COLLECTION}' ready with vector size {vector_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Upsert Embeddings into Qdrant\n",
    "Persist the vectors and metadata for each PDF page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [\n",
    "    PointStruct(\n",
    "        id=item[\"id\"],\n",
    "        vector=item[\"embedding\"],\n",
    "        payload={\n",
    "            \"pdf_file_name\": item[\"pdf_file_name\"],\n",
    "            \"page_index\": item[\"page_index\"],\n",
    "            \"image_path\": item[\"image_path\"],\n",
    "        },\n",
    "    )\n",
    "    for item in page_embeddings\n",
    "]\n",
    "\n",
    "operation_info = client.upsert(collection_name=QDRANT_COLLECTION, points=points)\n",
    "\n",
    "print(f\"Upserted {len(points)} points into collection '{QDRANT_COLLECTION}'.\")\n",
    "if points:\n",
    "    print(\"Example payload:\", points[0].payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieval Demo\n",
    "Search the Qdrant collection with a natural-language query and visualize matching pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def search_similar(query_text: str, top_k: int = 5):\n    processed_query = processor.process_queries([query_text])\n    processed_query = {k: v.to(device) for k, v in processed_query.items()}\n\n    with torch.no_grad():\n        outputs = model(**processed_query)\n\n    # Extract query embeddings (shape: [batch_size, num_tokens, embedding_dim])\n    if isinstance(outputs, torch.Tensor):\n        query_embedding_tensor = outputs\n    elif isinstance(outputs, (list, tuple)):\n        query_embedding_tensor = outputs[0]\n    elif hasattr(outputs, \"embeddings\"):\n        query_embedding_tensor = outputs.embeddings\n    elif hasattr(outputs, \"last_hidden_state\"):\n        query_embedding_tensor = outputs.last_hidden_state\n    else:\n        raise ValueError(\"Unexpected model output type for query\")\n\n    # Apply mean pooling across tokens to match the page embedding strategy\n    # query_embedding_tensor shape: [1, num_tokens, embedding_dim]\n    # pooled shape: [1, embedding_dim]\n    query_embedding_pooled = query_embedding_tensor.mean(dim=1)[0]\n    \n    # Convert to list for Qdrant\n    query_vector = query_embedding_pooled.detach().to(torch.float32).cpu().tolist()\n\n    results = client.search(\n        collection_name=QDRANT_COLLECTION,\n        query_vector=query_vector,\n        limit=top_k,\n        with_payload=True,\n    )\n\n    matches = []\n    for rank, result in enumerate(results, start=1):\n        payload = result.payload\n        image_path = payload[\"image_path\"]\n        with Image.open(image_path) as image:\n            plt.figure(figsize=(6, 8))\n            plt.imshow(image)\n            plt.axis(\"off\")\n            plt.title(f\"score={result.score:.4f} | page={payload['page_index']} | file={payload['pdf_file_name']}\")\n            plt.show()\n\n        matches.append(\n            {\n                \"rank\": rank,\n                \"score\": result.score,\n                \"payload\": payload,\n            }\n        )\n\n    return matches"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query; update with a relevant question for your PDF\n",
    "# matches = search_similar(\"What is the main topic of this document?\", top_k=3)\n",
    "# matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Answer Generation Stub\n",
    "Outline how to hand retrieved images to a vision-language model for question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision-Language Answering with Qwen2-VL\n",
    "\n",
    "After we retrieve the most relevant PDF page(s) using ColPali + Qdrant, we can ask a Vision-Language Model (VLM) to read that page image and answer the user's question.\n",
    "\n",
    "Why we need a VLM:\n",
    "- Our pipeline stores pages as images (not just plain text).\n",
    "- Sometimes OCR is missing or unreliable.\n",
    "- We want a natural-language answer grounded in the page content, including tables, diagrams, stamps, etc.\n",
    "\n",
    "Flow:\n",
    "1. User asks a question.\n",
    "2. We embed the question, search Qdrant, and get back the top matching page image.\n",
    "3. We send that page image + the user's question into a local Qwen2-VL model.\n",
    "4. The model generates an answer using ONLY what it \"sees\" in that page.\n",
    "\n",
    "This section shows how to load Qwen2-VL locally and run a question-answer step against the best-matching page.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# Configure device for VLM\n",
    "vlm_device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "VLM_MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"  # keep this as a string variable so I can change it later\n",
    "\n",
    "# Load processor / tokenizer / image processor\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_NAME)\n",
    "\n",
    "# Load model\n",
    "vlm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    VLM_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.backends.mps.is_available() else torch.float32,\n",
    "    device_map=None,\n",
    ")\n",
    "vlm_model = vlm_model.to(vlm_device)\n",
    "vlm_model.eval()\n",
    "\n",
    "print(\"VLM loaded on\", vlm_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def answer_question_with_vlm(query_text: str, retrieved_results):\n",
    "    \"\"\"\n",
    "    Use the local Qwen VLM to answer the user's query by looking directly at\n",
    "    the most relevant PDF page image we retrieved from Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    if not retrieved_results:\n",
    "        return \"No results found to answer from.\"\n",
    "\n",
    "    best = retrieved_results[0]\n",
    "    image_path = best[\"payload\"][\"image_path\"]\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant that answers questions using only the provided document page.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query_text},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    prompt = vlm_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = vlm_processor(\n",
    "        text=prompt,\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(vlm_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = vlm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "\n",
    "    generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    answer = vlm_processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: ask something about the document\n",
    "user_query = \"Summarize the main purpose of this document.\"\n",
    "\n",
    "retrieved = search_similar(user_query, top_k=3)\n",
    "print(\"Top match page index:\", retrieved[0][\"payload\"][\"page_index\"])\n",
    "print(\"Image path:\", retrieved[0][\"payload\"][\"image_path\"])\n",
    "\n",
    "final_answer = answer_question_with_vlm(user_query, retrieved)\n",
    "print(\"\\n=== Model Answer ===\\n\")\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "Release model resources so the notebook can be rerun without a kernel restart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del processor\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del page_embeddings\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del vlm_model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del vlm_processor\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del vlm_device\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "_ = torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"VLM + retrieval models cleaned up.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}