{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31153e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-core llama-index-embeddings-ollama qdrant-client IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG Playground: chunk → embed → Qdrant → retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load every dependency we need up front so the chunker, embedding model, and Qdrant client are ready for the rest of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import textwrap\n",
    "import atexit\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    PointStruct,\n",
    "    Filter,\n",
    "    FieldCondition,\n",
    "    MatchValue,\n",
    ")\n",
    "\n",
    "qdrant_client: QdrantClient | None = None\n",
    "embedding_model: OllamaEmbedding | None = None\n",
    "chunk_records: List[Dict[str, Any]] | None = None\n",
    "embedded_chunks: List[Dict[str, Any]] | None = None\n",
    "collection_name = \"video_chunks\"\n",
    "\n",
    "def cleanup_resources() -> None:\n",
    "    global qdrant_client, embedding_model, chunk_records, embedded_chunks\n",
    "    if qdrant_client is not None:\n",
    "        try:\n",
    "            qdrant_client.close()\n",
    "            print(\"Closed Qdrant client.\")\n",
    "        finally:\n",
    "            qdrant_client = None\n",
    "    if embedding_model is not None:\n",
    "        embedding_model = None\n",
    "        print(\"Released embedding model reference.\")\n",
    "    if embedded_chunks is not None:\n",
    "        print(f\"Releasing {len(embedded_chunks)} embedded chunks from memory.\")\n",
    "        embedded_chunks = None\n",
    "    if chunk_records is not None:\n",
    "        print(f\"Releasing {len(chunk_records)} chunk records from memory.\")\n",
    "        chunk_records = None\n",
    "\n",
    "atexit.register(cleanup_resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba4842",
   "metadata": {},
   "source": [
    "We define a small SaaS-style corpus covering pricing, onboarding, and license renewal scenarios so we can simulate multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the video transcript from the data directory\n",
    "with open('data/video.txt', 'r') as f:\n",
    "    video_transcript = f.read()\n",
    "\n",
    "documents = [\n",
    "    {\"doc_id\": \"video-transcript\", \"text\": video_transcript},\n",
    "]\n",
    "\n",
    "corpus = f\"[video-transcript]\\n{video_transcript}\"\n",
    "\n",
    "display(Markdown(f\"Loaded corpus with {len(documents)} documents.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chunk each document semantically so downstream retrieval works with coherent slices instead of arbitrary fixed windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=70,\n",
    "    embed_model=OllamaEmbedding(model_name=\"nomic-embed-text\"),\n",
    ")\n",
    "llama_documents = [\n",
    "    Document(text=doc[\"text\"], metadata={\"source_doc_id\": doc[\"doc_id\"]})\n",
    "    for doc in documents\n",
    "]\n",
    "nodes = splitter.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "chunk_records = []\n",
    "for node in nodes:\n",
    "    chunk_records.append(\n",
    "        {\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"text\": node.get_content(),\n",
    "            \"source_doc_id\": node.metadata.get(\"source_doc_id\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "display(Markdown(f\"Generated {len(chunk_records)} semantic chunks.\"))\n",
    "for idx, chunk in enumerate(chunk_records[:3], start=1):\n",
    "    preview = textwrap.shorten(chunk[\"text\"], width=200, placeholder=\"...\")\n",
    "    display(Markdown(f\"**Chunk {idx}** — source `{chunk['source_doc_id']}`\\n\\n{preview}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We embed each chunk with Ollama's local nomic-embed-text model so we can store vectors for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "embedded_chunks = []\n",
    "for record in chunk_records:\n",
    "    vector = embedding_model.get_text_embedding(record[\"text\"])\n",
    "    embedded_chunks.append(\n",
    "        {\n",
    "            \"chunk_id\": record[\"chunk_id\"],\n",
    "            \"source_doc_id\": record[\"source_doc_id\"],\n",
    "            \"text\": record[\"text\"],\n",
    "            \"vector\": vector,\n",
    "        }\n",
    "    )\n",
    "\n",
    "first_vector = embedded_chunks[0][\"vector\"]\n",
    "print(f\"Vector length: {len(first_vector)}\")\n",
    "print(\"First 5 values:\", [round(value, 4) for value in first_vector[:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a fresh Qdrant collection using cosine similarity so the entire notebook shares one vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "vector_size = len(embedded_chunks[0][\"vector\"])\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"Collection `{collection_name}` ready with vector size {vector_size}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upsert every embedded chunk into Qdrant, attaching the original text and source document metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [\n",
    "    PointStruct(\n",
    "        id=chunk[\"chunk_id\"],\n",
    "        vector=chunk[\"vector\"],\n",
    "        payload={\"text\": chunk[\"text\"], \"source_doc_id\": chunk[\"source_doc_id\"]},\n",
    "    )\n",
    "    for chunk in embedded_chunks\n",
    "]\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "print(f\"Inserted {len(points)} points into `{collection_name}`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We embed a sample question, retrieve similar chunks from Qdrant, and show how to apply an optional metadata filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What's the recommended RAG application DB to use ?\"\n",
    "query_vector = embedding_model.get_query_embedding(test_query)\n",
    "\n",
    "results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=3,\n",
    ")\n",
    "for idx, result in enumerate(results, start=1):\n",
    "    preview = textwrap.shorten(result.payload[\"text\"], width=200, placeholder=\"...\")\n",
    "    print(f\"Result {idx}\")\n",
    "    print(f\"  Score: {result.score:.4f}\")\n",
    "    print(f\"  Source doc: {result.payload.get('source_doc_id')}\")\n",
    "    print(f\"  Text: {preview}\")\n",
    "    print()\n",
    "\n",
    "filtered_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=3,\n",
    ")\n",
    "print(\n",
    "    f\"Filtered results (source_doc_id={embedded_chunks[0]['source_doc_id']}): {len(filtered_results)}\"\n",
    ")\n",
    "for idx, result in enumerate(filtered_results, start=1):\n",
    "    preview = textwrap.shorten(result.payload[\"text\"], width=200, placeholder=\"...\")\n",
    "    print(f\"  Result {idx} | Score: {result.score:.4f} | Text: {preview}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Brew venv)",
   "language": "python",
   "name": "jupyter-brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
